---
title: "<center>Лабораторна робота № 5 (6)</center>"
output:
  html_document:
    df_print: paged
fontsize: 14pt
editor_options:
  chunk_output_type: inline
---

**Тема.** Побудова моделей регресії

**Мета:** продемонструвати методику побудови моделі парної регресії під час обробки даних експерименту, набути навичок практичного застосування побудови моделі регресії.

<center>**Хід Роботи**</center>
Створення таблиці з даними
```{r}
my_data <- data.frame(
  X = c(-7.99, -5.50, -3.00, 0.00, 3.00, 5.50, 7.99),
  Y = c(8.28, 7.10, 4.50, 3.00, 4.10, 6.80, 8.20)   
)

print(my_data)
```
На основі цього набору будуємо лінійну модель:
```{r}
model <- lm(Y ~ X, data = my_data)

plot(my_data$X, my_data$Y, main = "Лінійна модель", xlab = "X", ylab = "Y", pch = 16)
abline(model, col = "blue")
```
Перевірка адекватності моделі
```{r}
summary(model)

par(mfrow = c(2, 2))
plot(model)
```
Будуємо поліноміальну модель
```{r}
poly_model <- lm(Y ~ poly(X, 2), data = my_data)

x_seq <- seq(min(my_data$X), max(my_data$X), length.out = 100)
pred <- predict(poly_model, newdata = data.frame(X = x_seq))

plot(my_data$X, my_data$Y, main = "Поліноміальна модель", xlab = "X", ylab = "Y", pch = 16)
lines(x_seq, pred, col = "red", lwd = 2)
```   
<center>**Відповідь на контрольні питання**</center>
*1. Запишіть систему нормальних рівнянь у матричному вигляді.*
Система нормальних рівнянь для лінійної регресії:
(X^T * X) * beta = X^T * y
Розв'язання:
beta = solve(t(X) %*% X) %*% t(X) %*% y

```{r}
X <- matrix(c(1, 1, 1, 1, 1, 0, 1, 2, 3, ), ncol = 2)
y <- c(1, 2, 3, 6, 8)

beta <- solve(t(X) %*% X) %*% t(X) %*% y
print(beta)
```

*2. Як визначається надійна зона регресії?*
```{r}
X <- c(1, 2, 3, 4, 5)  # Незалежні змінні
y <- c(2, 3, 5, 7, 11) # Залежна змінна

# Побудова лінійної моделі
model <- lm(y ~ X)

# Діапазон надійної зони
reliable_zone <- range(X)
print(reliable_zone)  # Надійна зона: від мінімального до максимального значення X
```
*3. Сформулюйте передумови класичної лінійної регресії.*
Передумови:
1. Лінійність: модель Y = beta0 + beta1 * X + epsilon
2. Незалежність залишків: кореляція між залишками має бути нульова
3. Нормальність залишків: залишки epsilon ~ N(0, sigma^2)
4. Гомоскедастичність: Var(epsilon) = const
5. Відсутність мультиколінеарності між X
6. Незалежність помилок від змінних X

```{r}
# Перевірка деяких передумов в R:
model <- lm(y ~ X)

# Графік залишків для перевірки гомоскедастичності
plot(model, which = 1)  # Діаграма "Залишки проти передбачених значень"

# Перевірка нормальності залишків
plot(model, which = 2)  # Q-Q графік
```

*4. Коли застосовується зважений метод найменших квадратів? У чому його сенс?*
Приклад WLS для моделювання із гетероскедастичністю

```{r}
#Створюємо дані
X <- c(1, 2, 3, 4, 5)
y <- c(1.2, 2.3, 3.1, 3.8, 5.1)
weights <- c(1, 0.5, 0.2, 0.1, 0.05)  # Ваги (обернені до дисперсій залишків)

#Побудова WLS-моделі
model_wls <- lm(y ~ X, weights = weights)

#Перегляд результатів
summary(model_wls)

#Графічний аналіз
plot(X, y, main = "Зважена регресія", xlab = "X", ylab = "Y", pch = 16)
abline(model_wls, col = "red")
```